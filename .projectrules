# Project Rules - Filmer Video Processing

## Project Purpose
This project processes video files by:
1. Cutting specific segments from input videos based on timeline configuration
2. Concatenating selected segments
3. Combining the result with narration audio tracks
4. Outputting final edited videos

## Development Guidelines

### Code Organization
- Keep main processing logic in `cut_video.py`
- Store configuration in JSON files
- Separate input, narration, and output files in dedicated folders

### Configuration Management
- Use `timeline.json` for main project configuration
- Each timeline entry defines:
  - Input video path
  - Narration audio path
  - Output video path
  - Array of time segments to extract

### Video Processing Rules
- Segments are defined by start and end times in seconds (float)
- Segments are trimmed and concatenated in order
- Video segments are processed without audio initially
- Final audio comes from narration file
- Use `-shortest` flag to match output duration to shortest input

### File Naming Conventions
- Input videos: Keep original names
- Narration files: Use descriptive names (e.g., `imdb123123.m4a`)
- Output files: Use descriptive names (e.g., `output_cut.mp4`, `output_cut2.mp4`)

### Dependencies
- Python 3.x
- FFmpeg (configured path in code)
- Standard library: `json`, `subprocess`

### Testing
- Test with small segments first
- Verify output video quality
- Check audio synchronization
- Ensure all segments are included in final output

### Maintenance
- Update FFmpeg path if installation changes
- Keep JSON files valid and well-formatted
- Document any changes to filter complex operations
- Backup important configuration files

---

## System Architecture: Semantic Timecode Engine

### Overview
This project is building a **Semantic Timecode Engine** that uses semantic search to match narration text with video segments.

### System Flow
```
Movie SRT (2h)
â†“
Chunker (10â€“20 sec / 100â€“200 words)
â†“
Embedding Model (fixed)
â†“
ChromaDB (with time metadata)
â†“
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Narration SRT (10â€“20 min)
â†“
Sentence / Paragraph Grouper
â†“
Embedding (same model)
â†“
Semantic Search
â†“
Timeline JSON Generator
â†“
FFmpeg / Premiere Pipeline
```

This architecture is **production-ready and correct**.

---

## ChromaDB Data Model Design

### Golden Rule
> **Each record = a specific time range from the movie + subtitle text for that range**

### Collection Structure
```
collection: movie_subtitles_<movie_id>
```

### Document Schema
```json
{
  "id": "movie_000123",
  "document": "Subtitle text 100-200 words",
  "embedding": [...],
  "metadata": {
    "movie_id": "tt0133093",
    "start_time": 2450.32,
    "end_time": 2470.85,
    "duration": 20.53,
    "srt_start": "00:40:50,320",
    "srt_end": "00:41:10,850"
  }
}
```

### Why This Design is Correct
- Timecode is always attached to embedding
- For JSON output, only metadata needs to be read
- No joins or additional calculations needed

---

## Stage 1: Full Movie Subtitle Processing (Offline - One Time)

### Input
- Complete movie SRT file (2 hours)

### Processing Steps

#### 1.1 Parse SRT
Each item format:
```
index
start --> end
text
```

#### 1.2 Accumulate into Chunks
Chunking rules:
- **Minimum**: 10 seconds
- **Maximum**: 20 seconds
- OR:
  - Minimum 100 words
  - Maximum 200 words

> Priority is time-based, not sentence-based

#### 1.3 Build Final Chunk
```python
chunk = {
  "text": combined_text,
  "start_time": first.start,
  "end_time": last.end
}
```

#### 1.4 Generate Embedding
- Use **one fixed model**
- English â†’ English (high accuracy)
- If Persian: use Bridge Translation later

#### 1.5 Insert into ChromaDB
- Unique ID
- Metadata includes start/end times

ðŸ“Œ This stage runs **only once** per movie.

---

## Stage 2: Narration Processing (Online / Runtime)

### Input
- Narration SRT file (10â€“20 minutes)

### Correct Search Strategy

#### Wrong Approach (Don't Do)
- Single sentence search

#### Correct Approach
- **Sliding Window**
- Group 1 or 2 sentences together

Example:
```
Sentence 1 + Sentence 2
Sentence 2 + Sentence 3
```

This provides:
- Better context
- More stable matches

### ChromaDB Query
```python
results = collection.query(
  query_embeddings=[narration_embedding],
  n_results=3
)
```

### Output Format
```json
{
  "ids": ["movie_000123"],
  "metadatas": [{
    "start_time": 2450.32,
    "end_time": 2470.85
  }],
  "distances": [0.18]
}
```

---

## Final JSON Generation (Direct Connection to Previous Project)

### Selection Rules
- Apply similarity threshold
- Remove severe overlaps
- Merge nearby segments

### Final Output JSON
```json
{
  "input": "movie.mp4",
  "narration": "narration.mp3",
  "output": "final.mp4",
  "segments": [
    {
      "start": 2450.32,
      "end": 2470.85,
      "score": 0.82
    },
    {
      "start": 3120.10,
      "end": 3142.00,
      "score": 0.79
    }
  ]
}
```

ðŸ“Œ This JSON connects **without any changes** to the same FFmpeg pipeline we built.

---

## API Router Design

### Suggested Endpoints
```
POST /embed/movie
POST /query/narration
GET  /health
```

### Important Parameters
- token
- model_name
- movie_id

### Why API?
- Future UI integration
- Future batch processing
- Future worker architecture

---

## Technical Decisions (Confirmed)

âœ” Time + word-based chunking â†’ Correct
âœ” ChromaDB â†’ Correct
âœ” Fixed embedding model â†’ Very important
âœ” Store start/end times â†’ Critical
âœ” Independent JSON output â†’ Excellent
âœ” Direct connection to FFmpeg â†’ Production-ready

---

## Recommended Development Order

When continuing development, follow this order:

1. **Clean SRT Chunker (with tests)**
2. ChromaDB schema + insert
3. Narration sliding-window search
4. Overlap resolver
5. Final JSON generator
6. Real accuracy benchmark

This project, if continued this way, can easily become:
- A product
- A service
- The core of an analytical video production platform

